\section{Approach}
\subsection{Motivation for Monte Carlo in Spark}
% Why Monte Carlo is a promising approach for memory-efficiency.
The Monte Carlo method is an alternative approach to the traditional PageRank implementation. Instead of iteratively computing the entire rank vector, it focuses on simulating random walks on the graph. In this approach, a specified number of walkers are released per node, and they then traverse the graph according to the "random surfer" model. Similar to the original algorithm, a random surfer follows an outgoing link with probability $\alpha$ or teleports to a random node in the graph with probability $1-\alpha$. Ultimately, the PageRank values are estimated by the relative frequency with which a node was visited by random walkers. The key difference from the iterative approach is that the walkers take a predefined number of steps instead of updating the rank vector until convergence. Thus, the Monte Carlo approach can control memory usage by limiting the number of walkers and steps, making it a promising candidate for approximating the PageRank of large-scale graphs \cite{avrachenkov_monte_2007}. \par
Additionally, estimating PageRank values is sufficient for many applications. For real-world tasks such as website ranking or user recommendation on social networks, the only things of interest are a node's relative importance and the stability of the ranking. Therefore, total convergence of the PageRank vector is unnecessary. The approximate Monte Carlo approach achieves significant memory and computational savings with only a slight loss in accuracy. This balance between performance and accuracy enables graph analytics on graphs that would otherwise be too large to handle.\par
Furthermore, implementing Monte Carlo PageRank in a Spark environment utilizes a distributed data processing framework that includes Spark's Resilient Distributed \allowbreak Datasets (RDDs). This data structure can represent the entire graph and the walker's state at every step. Spark's properties, such as fault tolerance through lineage and efficient in-memory computation, make it a more attractive platform than the iterative PageRank method for evaluating the Monte Carlo PageRank method. Of particular interest is whether the Monte Carlo approach can operate under limited memory conditions while achieving sufficient accuracy and maintaining low variance throughout the ranking. \par
This thesis explores the integration of such an approximate Monte Carlo method in the Spark environment and evaluates it's performance and accuracy on limited memory usage in comparison to the standard GraphX PageRank method.
 

\subsection{System Architecture and Data Flow}
% Describe how RDDs are used for graph and walker states.
The Monte Carlo Implementation is structured around two main components: The graph representation and the walker states. Both of them are stored in Sparks RDDs, which make them suitable for processing in a distributed system. \par
At the beginning of the program a given edge list file is red to get the structure of the graph. It is stored as an RDD of directed edges. Then by grouping the outgoing neighbors of each node an adjacency list is constructed. Additionally, an array of all vertices is required to support teleportation. This list will be distributed as a broadcasting variable to all nodes. This mitigates costly joins during every teleportation. Because the set of nodes is required in every simulation step it is cached in memory for efficient access. \par
A fixed number of walkers are initialized and represented in a seperate RDD. The RDD is distributed across multiple partitions to ensure efficient parallel processing and scaling when analyzing large graphs. In the core part, the walker simulation, a new walker RDD is created within every step with updated walker positions. To control memory usage, the previous walker RDD is explicitly unpersisted after the new step has been materialized. \par
Finally, the PageRank values are estimated based on the visits of a node, normalized by the total number of walkers. 

% Each entry in the RDD is a random vertex ID, representing the current vertex of a walker. In each simulation step the walker RDD is joined with the adjacency list to get all possible outgoing edges. Then according to the random surfer model, the walker probabilistically decides to take an outgoing edge or to jump to a random node. To ensure deterministic behavior each walker uses a seeded random number generator. In case of a dangling node the walker is always teleporting to a random node. \par
% After each walker updates its position, a new walker RDD is initialized with the updated positions. Since Spark RDDs are lazy, the next position must be materialized before the previous walker RDD is explicitly unpersisted. This ensures that only the current walker RDD and the adjacency list occupy memory. \par 
% Finally, once all steps have been completed, the approximate PageRank values are computed by counting each node's visits and then normalizing the count by the total number of walkers. 



\subsection{Monte Carlo Algorithm Design}
% Step-by-step random walk simulation, damping, teleportation.

The Monte Carlo PageRank algorithm is a step by step simulation that follows the random surfer model. 
The pseudo code bellow describes the algorithm used in this thesis. 

\vspace{0.5em}
\begin{algorithm}[H]
\caption{Monte Carlo PageRank Approximation}
\KwIn{Graph $G=(V,E)$, walkers per node $w$, number of steps $k$, damping factor $\alpha$}
\KwOut{Approximate PageRank scores $\hat{\pi}(v)$ for all $v \in V$}

\ForEach{node $v \in V$}{
    Initialize $w$ walkers at $v$
}
\For{step = 1 to $k$}{
    \ForEach{walker $i$ at node $u$}{
        Generate random number $r \in [0,1]$\;
        \eIf{$r < \alpha$ \textbf{and} $u$ has outlinks}{
            Move walker to a random neighbor of $u$\;
        }{
            Teleport walker to a random node $v \in V$\;
        }
    }
}
\ForEach{node $v \in V$}{
    $\hat{\pi}(v) \gets \frac{\text{Number of walkers at } v}{|V| \cdot w}$
}
\end{algorithm}
\vspace{0.5em}

First, the algorithm first initializes $w$ walkers per node, which is resulting in a total number of $w\cdot |V|$ walkers. Each walker is randomly distributed throughout the graph. Then, each walker simultaneously performs a number of $k$ independent steps. \par
At each step, the walker $w$ generates a random number between 



\subsection{Spark Implementation Details}


\subsection{Memory Management Strategy}
% Describe unpersisting, persisting, and optimizations.

 